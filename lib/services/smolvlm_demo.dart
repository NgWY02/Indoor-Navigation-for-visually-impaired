/// Demo/Test file showing how SmolVLM works
/// This demonstrates the difference between real VLM comparison vs fake embeddings

import 'dart:typed_data';
import 'smolvlm_service.dart';

class SmolVLMDemo {
  final SmolVLMService _service;
  
  SmolVLMDemo(String serverUrl) : _service = SmolVLMService(baseUrl: serverUrl);
  
  /// Example workflow: Admin creates a location
  Future<void> demoLocationCreation(Uint8List imageBytes) async {
    print("=== ADMIN: Creating Location ===");
    
    // Method 1: Get direct text description (RECOMMENDED)
    try {
      final description = await _service.generateDescription(imageBytes);
      print("‚úÖ SmolVLM Description: $description");
      
      // This description gets stored in database for comparison
      print("üíæ Storing description in database...");
      
    } catch (e) {
      print("‚ùå Error: $e");
    }
    
    // Method 2: Legacy embedding method (for compatibility)
    try {
      final fakeEmbedding = await _service.generateEmbedding(imageBytes);
      print("‚ö†Ô∏è  Fake embedding (${fakeEmbedding.length} dimensions): ${fakeEmbedding.take(5).toList()}...");
      print("   Note: This is NOT a real neural embedding, just a hash of the text description!");
      
    } catch (e) {
      print("‚ùå Embedding error: $e");
    }
  }
  
  /// Example workflow: User navigation recognition
  Future<void> demoLocationRecognition(Uint8List liveImageBytes, String storedDescription) async {
    print("\n=== USER: Navigation Recognition ===");
    
    // Method 1: Direct VLM comparison (RECOMMENDED)
    try {
      final similarity = await _service.compareImageWithDescription(liveImageBytes, storedDescription);
      print("‚úÖ SmolVLM Similarity Score: $similarity");
      
      if (similarity > 0.7) {
        print("üéØ LOCATION IDENTIFIED! (${(similarity * 100).toStringAsFixed(1)}% match)");
      } else if (similarity > 0.4) {
        print("ü§î Possible match (${(similarity * 100).toStringAsFixed(1)}% similarity)");
      } else {
        print("‚ùå No match (${(similarity * 100).toStringAsFixed(1)}% similarity)");
      }
      
    } catch (e) {
      print("‚ùå Comparison error: $e");
    }
  }
  
  /// Show the difference between approaches
  void explainApproaches() {
    print("\n=== COMPARISON OF APPROACHES ===");
    print("üìä Traditional Computer Vision:");
    print("   ‚Ä¢ Extract feature vectors (embeddings) from images");
    print("   ‚Ä¢ Compare vectors using cosine similarity");
    print("   ‚Ä¢ Works well for objects, but struggles with complex scenes");
    print("");
    print("üß† SmolVLM Vision-Language Model:");
    print("   ‚Ä¢ Understands images like humans do");
    print("   ‚Ä¢ Generates rich text descriptions");
    print("   ‚Ä¢ Compares images semantically, not just visually");
    print("   ‚Ä¢ Better for indoor navigation with furniture, signs, layout");
    print("");
    print("‚ö° Your Implementation:");
    print("   ‚Ä¢ Uses SmolVLM for location understanding");
    print("   ‚Ä¢ Stores text descriptions (not real embeddings)");
    print("   ‚Ä¢ Direct image-to-description comparison");
    print("   ‚Ä¢ Fake embeddings only for database compatibility");
  }
}

/// Usage example
void main() async {
  final demo = SmolVLMDemo('http://localhost:8080');
  
  // Demo explanation
  demo.explainApproaches();
  
  // Would normally load real image bytes here
  // final imageBytes = await loadImageFromCamera();
  // await demo.demoLocationCreation(imageBytes);
  // await demo.demoLocationRecognition(imageBytes, "A library room with wooden tables...");
}
